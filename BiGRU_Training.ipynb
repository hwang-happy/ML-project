{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Poniższy notebook ma za zadanie nauczyć sieć BiGRU o ustalonych w notebooku BiGRU_hyper_parametr_opt hiperparametrach na zbiorze złożonym z około 67 tys sekwencji aminokwasowych (ok. 14 mln aminokwasów), a następnie sprawdzić jej skuteczność (procentowa liczba dobrych predykcji) oraz precision/recall dla poszczególnych struktur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/kacper/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, RepeatVector\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_depth = 20\n",
    "proteins = ['*']*int(mem_depth/2)\n",
    "structures = ['*']*int(mem_depth/2)\n",
    "\n",
    "\n",
    "path = \"output102361.out\"\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        splited = line.strip().split(' ')\n",
    "        if len(list(splited[1])) == len(list(splited[2])):\n",
    "            proteins.extend(list(splited[1].upper()))\n",
    "            proteins.extend(['*']*int(mem_depth/2))\n",
    "            structures.extend(list(splited[2].upper()))\n",
    "            structures.extend(['*']*int(mem_depth/2))\n",
    "        \n",
    "            \n",
    "            \n",
    "alphabet_structures = ['C', 'H', 'E', 'T', \"*\"]\n",
    "alphabet_proteins = ['A','R','N','D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', '*']\n",
    "\n",
    "known_proteins = []\n",
    "known_structures = []\n",
    "for i in range(len(proteins)):\n",
    "    if proteins[i] in alphabet_proteins and structures[i] in alphabet_structures:\n",
    "        known_proteins.append(proteins[i])\n",
    "        known_structures.append(structures[i])\n",
    "proteins = known_proteins\n",
    "structures = known_structures\n",
    "\n",
    "structures_indices = dict((c, i) for i, c in enumerate(alphabet_structures))\n",
    "indices_structures = dict((i, c) for i, c in enumerate(alphabet_structures))\n",
    "\n",
    "proteins_indices = dict((c, i) for i, c in enumerate(alphabet_proteins))\n",
    "indices_proteins = dict((i, c) for i, c in enumerate(alphabet_proteins))\n",
    "\n",
    "\n",
    "#Get time series\n",
    "protein_blocks = []\n",
    "structure = []\n",
    "for i in range(0, len(proteins) - mem_depth + 1):\n",
    "    protein_blocks.append(proteins[i: i + mem_depth])\n",
    "    structure.append(structures[i + int(mem_depth/2) - 1])\n",
    "    \n",
    "\n",
    "#Vectorisation\n",
    "X = np.zeros((len(protein_blocks), mem_depth, len(alphabet_proteins)), dtype=np.bool)\n",
    "y = np.zeros((len(structure), len(alphabet_structures)), dtype=np.bool)\n",
    "for i, block in enumerate(protein_blocks):\n",
    "    for t, protein in enumerate(block):\n",
    "        X[i, t, proteins_indices[protein]] = 1\n",
    "    y[i, structures_indices[structure[i]]] = 1\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(64), input_shape=(mem_depth, len(alphabet_proteins))))\n",
    "model.add(Dense(len(alphabet_structures)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13974594/13974594 [==============================] - 4854s - loss: 0.9025  \n",
      "Epoch 2/5\n",
      "13974594/13974594 [==============================] - 4939s - loss: 0.8547  \n",
      "Epoch 3/5\n",
      "13974594/13974594 [==============================] - 5065s - loss: 0.8454  \n",
      "Epoch 4/5\n",
      "13974594/13974594 [==============================] - 5030s - loss: 0.8414  \n",
      "Epoch 5/5\n",
      "13974594/13974594 [==============================] - 5042s - loss: 0.8391  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07e6be42e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, train_size=0.66)\n",
    "\n",
    "train_filtered_x = np.array([X_train[i] for i in range(len(X_train)) if alphabet_structures[np.argmax(y_train[i])] != '*'])\n",
    "train_filtered_y = np.array([y_train[i] for i in range(len(y_train)) if alphabet_structures[np.argmax(y_train[i])] != '*'])\n",
    "\n",
    "test_filtered_x = np.array([X_test[i] for i in range(len(X_test)) if alphabet_structures[np.argmax(y_test[i])] != '*'])\n",
    "test_filtered_y = np.array([y_test[i] for i in range(len(y_test)) if alphabet_structures[np.argmax(y_test[i])] != '*'])\n",
    "\n",
    "model.fit(train_filtered_x, train_filtered_y, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13974594/13974594 [==============================] - 44158s - loss: 0.8375 \n",
      "Epoch 2/5\n",
      "13974594/13974594 [==============================] - 5101s - loss: 0.8362  \n",
      "Epoch 3/5\n",
      "13974594/13974594 [==============================] - 5554s - loss: 0.8353  \n",
      "Epoch 4/5\n",
      "13974594/13974594 [==============================] - 5424s - loss: 0.8344  \n",
      "Epoch 5/5\n",
      "13974594/13974594 [==============================] - 5230s - loss: 0.8351  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0cee63beb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_filtered_x, train_filtered_y, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Saving trained model\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"BiGRU.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "    \n",
    "model.save_weights(\"BiGRU.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting\n",
    "predictions = (model.predict(test_filtered_x))\n",
    "#Getting structure codes\n",
    "predictions = np.array([alphabet_structures[np.argmax(prediction)] for prediction in predictions])\n",
    "#Getting structure codes\n",
    "structures = np.array([alphabet_structures[np.argmax(y)] for y in test_filtered_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66463244956326817"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting accuracy\n",
    "acc_arr = predictions == structures\n",
    "np.mean(acc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Precision: 0.6231842161956342\n",
      "C Recall: 0.6054776716609714\n"
     ]
    }
   ],
   "source": [
    "C_arr_pred = predictions == \"C\"\n",
    "C_arr_struct = structures == \"C\"\n",
    "C_precision = sum(C_arr_pred * acc_arr)/sum(C_arr_pred)\n",
    "C_recall = sum(C_arr_pred * acc_arr)/sum(C_arr_struct)\n",
    "print(\"C Precision: {0}\".format(C_precision))\n",
    "print(\"C Recall: {0}\".format(C_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H Precision: 0.7096588454494616\n",
      "H Recall: 0.842855394698064\n"
     ]
    }
   ],
   "source": [
    "H_arr_pred = predictions == \"H\"\n",
    "H_arr_struct = structures == \"H\"\n",
    "H_precision = sum(H_arr_pred * acc_arr)/sum(H_arr_pred)\n",
    "H_recall = sum(H_arr_pred * acc_arr)/sum(H_arr_struct)\n",
    "print(\"H Precision: {0}\".format(H_precision))\n",
    "print(\"H Recall: {0}\".format(H_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Precision: 0.6624632165823134\n",
      "E Recall: 0.6477236271411647\n"
     ]
    }
   ],
   "source": [
    "E_arr_pred = predictions == \"E\"\n",
    "E_arr_struct = structures == \"E\"\n",
    "E_precision = sum(E_arr_pred * acc_arr)/sum(E_arr_pred)\n",
    "E_recall = sum(E_arr_pred * acc_arr)/sum(E_arr_struct)\n",
    "print(\"E Precision: {0}\".format(E_precision))\n",
    "print(\"E Recall: {0}\".format(E_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Precision: 0.5469333950701863\n",
      "T Recall: 0.2869914355521893\n"
     ]
    }
   ],
   "source": [
    "T_arr_pred = predictions == \"T\"\n",
    "T_arr_struct = structures == \"T\"\n",
    "T_precision = sum(T_arr_pred * acc_arr)/sum(T_arr_pred)\n",
    "T_recall = sum(T_arr_pred * acc_arr)/sum(T_arr_struct)\n",
    "print(\"T Precision: {0}\".format(T_precision))\n",
    "print(\"T Recall: {0}\".format(T_recall))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
